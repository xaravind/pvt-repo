#  Full Project Setup Guide (GitHub → Docker → EC2 → EKS)

---

## 📝 Agenda

This project walks you through the complete process of:

* ✅ Creating a private GitHub repository and pushing code with a Dockerfile
* ✅ Setting up an AWS EC2 instance
* ✅ Installing Docker, building images, and pushing them to Docker Hub
* ✅ Assigning IAM roles and preparing EC2 for EKS operations
* ✅ Installing and configuring EKS and deploying a containerized app
* ✅ Understanding rolling updates and cleaning up resources

---

## ✅ First Stage: GitHub Setup

1. Login to your GitHub account.

`  ![Image](https://github.com/user-attachments/assets/74e6f5b7-e71a-45cc-ae20-7361f5737e16)

2. Create a **private repository**.

  ![Image](https://github.com/user-attachments/assets/dcbd22ee-c1fb-4b3c-9eed-4eaadaa35af2)

3. Go to the top right of your profile → **Settings** → **Developer Settings** → **Personal Access Tokens (PAT)**:

   * Select **Tokens (Classic)** → Click **Generate new token** → Choose **Generate new classic token**.
   * Set an expiry based on your choice and **grant only repo access**.
   * Scroll down and click on **Generate token**.

  ![Image](https://github.com/user-attachments/assets/a4acdeae-ad2f-4e0f-b784-551760f78074)

  ![Image](https://github.com/user-attachments/assets/a0e7731d-ff39-4522-9c21-434fcebea2f2)

  ![Image](https://github.com/user-attachments/assets/ad0d690a-0a94-43fc-9919-ea46c874d358)

  ![Image](https://github.com/user-attachments/assets/a8458f62-ac9c-4bf9-b338-d220e9a581b7)

  ![Image](https://github.com/user-attachments/assets/d0c2bf67-e800-4ebc-8dce-d4e4dd82a887)

4. **Copy the token** and save it somewhere safe.

  ![Image](https://github.com/user-attachments/assets/8880920a-b16f-4826-a440-ab331f0e1b02)
  
   **Note:** Never ever share your credentials or tokens with anyone.

5. Go to your private repository and **copy the Git URL**.
  
  ![Image](https://github.com/user-attachments/assets/c7db7a28-578b-4cc2-a42b-70c106740f7a)

6. Open your **VS Code or local terminal** and **clone the repo using the token**.

  ![Image](https://github.com/user-attachments/assets/9d6545cf-a45f-4d3d-b7df-5e82ac821351)

    ```bash
    aravi@Aravind MINGW64 ~/11_04
    $ git clone https://github.com/xaravind/pvt-repo.git
    Cloning into 'pvt-repo'...
    remote: Enumerating objects: 6, done.
    remote: Counting objects: 100% (6/6), done.
    remote: Compressing objects: 100% (3/3), done.
    remote: Total 6 (delta 0), reused 3 (delta 0), pack-reused 0 (from 0)
    Receiving objects: 100% (6/6), done.
    ```
7. Copy your project code into the **cloned Git folder**.

  ![Image](https://github.com/user-attachments/assets/4b491638-612c-4677-bb56-90c5d9caa07f)


8. Write a `Dockerfile` in your project, save it, and **push the code**.

  ![Image](https://github.com/user-attachments/assets/3c78269d-b5ee-40c0-b0cb-753b13a384d5)

9. Open GitHub in your browser and check the files. You should see everything updated.

  ![Image](https://github.com/user-attachments/assets/b09c5774-a044-4578-8594-d22f5a56cfec)

> ✅ We have now created a GitHub repo, generated a PAT key, cloned the repo locally, added code, created a Dockerfile, and pushed files to GitHub.

---

## ✅ Second Stage: Launch EC2 in AWS

1. Login to your **AWS account**.

  ![Image](https://github.com/user-attachments/assets/81ef1385-faa3-4085-bcee-8067a971cfc9)
  
2. Go to **EC2**, click **Instances**, and then click **Launch Instance**.

  ![Image](https://github.com/user-attachments/assets/49a1605d-42d4-48e1-9e06-4fde4f26f544)

3. Provide a name and **create a new key pair**.

  ![Image](https://github.com/user-attachments/assets/595b2f27-a048-442b-a9cd-b05d57feda9d)

  ![Image](https://github.com/user-attachments/assets/7f79257f-b292-4154-af03-0cc5e663690c)

  ![Image](https://github.com/user-attachments/assets/1ad10bfa-8092-4f05-bfe8-bd61aa697ce0)

   Keep all other settings as **default**.
   This VM will be used to build images, create EKS clusters, and communicate with the cluster.
4. Choose **Amazon Linux AMI** (which is based on RHEL) and click **Launch instance**.
5. Copy the **public IP** and connect using **PuTTY or MobaXterm** (your choice).

  ![Image](https://github.com/user-attachments/assets/45990fe0-0282-4eef-a9c9-aeb2a5ee2e35)

  ![Image](https://github.com/user-attachments/assets/d9a6bd42-f9cc-42c8-a250-58739c28b610)

6. Switch to root user and **update the server**:

   ```bash
   sudo su -
   yum update -y
   ```

> ✅ In this stage, we created an EC2 instance, generated a `.pem` key, launched the instance with default settings, connected to it, and updated the server.

---

## ✅ Third Stage: Docker Setup & Push Image

1. Install Git and Docker, then start Docker:

   ```bash
   yum install -y git docker
   systemctl start docker
   ```
2. Clone the **private repo** using your GitHub username and the **PAT key** as password.
3. Switch to the project directory and **build the Docker image**.
4. Check the image and run the container:

   ```bash
   docker ps
   docker run -dt --name jscode -p 80:80 jscode:1.0
   ```

  ![Image](https://github.com/user-attachments/assets/b2a43a19-4452-48eb-8c75-bab97d2f2daa)

   > ✅ Our Docker image is just **56.2MB** as we used the Alpine image (lightweight).
Here’s an improved version of your steps (5–8) with emphasis added on **real-world relevance** and an **interview-style explanation** for the scenario you mentioned:

---

5. Go **inside the running container** and **manually update files** like `index.html` to simulate version changes (e.g., v1 → v2).

   > ✅ This is useful in **emergency situations** where you need to quickly patch or debug an application without restarting the container or rebuilding the image.

    ```bash
    [root@ip-172-31-86-127 js_code]# docker exec -it 224e0843b7 sh
    / # cd /usr/share/nginx/html
    /usr/share/nginx/html # ls
    50x.html    bundle.js   images      index.html
    /usr/share/nginx/html # >index.html
    /usr/share/nginx/html # vi index.html
    ```

6. To access the app using `<public-ip>:80`, make sure to **modify the EC2 security group** and **add port 80** to inbound rules.

7. Once your changes are tested inside the container, you can **create a new Docker image with all modifications**.
   This step is **very important** in real-world scenarios where you tweak live containers and want to preserve the state.

    ```bash
    [root@ip-172-31-86-127 js_code]# docker commit 224e0843b751 jscode:2.0
    sha256:afa8f59407427d1339a6140c73cdcdb4ca1153437ab007eb21b4e02980740cb5
    [root@ip-172-31-86-127 js_code]# docker images
    REPOSITORY   TAG       IMAGE ID       CREATED          SIZE
    jscode       2.0       afa8f5940742   5 seconds ago    56.2MB
    jscode       1.0       b2d1f7cddd1e   13 minutes ago   56.2MB
    [root@ip-172-31-86-127 js_code]# docker ps
    CONTAINER ID   IMAGE        COMMAND                  CREATED          STATUS          PORTS                               NAMES
    224e0843b751   jscode:1.0   "/docker-entrypoint.…"   11 minutes ago   Up 11 minutes   0.0.0.0:80->80/tcp, :::80->80/tcp   jscode
    ```

   #### 📌 Interview Scenario:

   **Q: I did some modifications in a running container. Can I create a Docker image with those changes? If yes, how?**
   ✅ **Yes, it’s possible.**
   Here's how you do it:

   ```bash
   docker commit <container_id> <new_image_name>:<tag>
   ```

   **Example:**

   ```bash
   docker commit jscode xaravind/jscode:2.0
   ```

   This creates a new image from the **current state** of the running container, including all manual edits or installed packages.

8. Now login to **Docker Hub**, and create a **private repository** to store your updated Docker images.

  ![Image](https://github.com/user-attachments/assets/9fedc90f-c308-4820-ad8f-751406492ae8)

  ![Image](https://github.com/user-attachments/assets/545bf420-3a73-4fef-82b8-1bbbc13a1459)

---


9. Tag images for pushing:

   ```bash
   docker tag jscode:1.0 xaravind/jscode:v1
   docker tag jscode:2.0 xaravind/jscode:v2
   ```
10. Login to Docker using your credentials.
    This will create `/root/.docker/config.json` which stores credentials.
    We will use this file to pull images in Kubernetes.
    ```bash
    [root@ip-172-31-86-127 js_code]# docker login
    Log in with your Docker ID or email address to push and pull images from Docker Hub. If you don't have a Docker ID, head over to https://hub.docker.com/ to create one.
    You can log in with your password or a Personal Access Token (PAT). Using a limited-scope PAT grants better security and is required for organizations using SSO. Learn more at https://docs.docker.com/go/access-tokens/

    Username: xaravind
    Password:
    WARNING! Your password will be stored unencrypted in /root/.docker/config.json.
    Configure a credential helper to remove this warning. See
    https://docs.docker.com/engine/reference/commandline/login/#credentials-store

    Login Succeeded

    ```

11. Push the tagged images to Docker Hub:

    ```bash
    docker push xaravind/jscode:v1
    docker push xaravind/jscode:v2
    ```
12. Open Docker Hub in browser and **verify images are uploaded**.


  ![Image](https://github.com/user-attachments/assets/6fc00a38-1288-4c9a-9604-51f5626a36d4)

> ✅ In this stage, we installed packages, cloned the repo, built Docker images, ran and modified containers, and pushed final images to Docker Hub.

---

## ✅ Fourth Stage: EKS Setup with IAM Role

1. Go to AWS Console → Search for **IAM**.

  ![Image](https://github.com/user-attachments/assets/42642131-9969-496f-90f7-cfdc4bd92525)

2. Click **Roles** → **Create role**.

  ![Image](https://github.com/user-attachments/assets/332a1c05-8791-43c5-aeb5-2f9e67cd61ca)

3. Select **Trusted entity type: AWS Service** and use case as **EC2**, then click **Next**.

  ![Image](https://github.com/user-attachments/assets/f3519430-8ca7-43ae-a3ca-cf5eb85287ca)

4. Choose **AdministratorAccess** policy (has full EC2 + EKS permissions), then click **Next**.

  ![Image](https://github.com/user-attachments/assets/2d529be2-ce5d-44f2-8fa9-c39ff463c55a)

5. Name the role and click **Create Role**.

  ![Image](https://github.com/user-attachments/assets/76737ccf-9282-4ec9-8e37-6c40f65a4c82)

6. Go to **EC2 Instances**, select your instance → **Actions** → **Security** → **Modify IAM role**.

  ![Image](https://github.com/user-attachments/assets/54c00781-bfde-487d-b606-939425e20135)

7. Attach the newly created **IAM role** to the instance.

  ![Image](https://github.com/user-attachments/assets/d06ea602-d9c7-46df-94c6-35b3171dd388)

> ✅ Now your EC2 instance can control AWS services via AWS CLI.

8. AWS CLI comes pre-installed in Amazon Linux.
   For other OS, you’ll need to install it manually.

9. Check AWS access:

   ```bash
   aws s3 mb s3://test-k8s.js
   aws s3 ls
   ```

10. Create an `eks.yml` file locally and push it to GitHub.

  ![Image](https://github.com/user-attachments/assets/e71ec6ba-9475-4f40-9b99-f2d4023a0e6f)

    > We are using **spot instances** for cost-saving (\~90% discount), but they may be terminated with short notice.

    ```bash
    apiVersion: eksctl.io/v1alpha5
    kind: ClusterConfig

    metadata:
    name: project
    region: us-east-1

    managedNodeGroups:
    - name: project
    instanceType: m5.large
    desiredCapacity: 3
    spot: true
    ```

11. Pull the updated code into the EC2 instance.

    ```bash
    [root@ip-172-31-86-127 ~]# cd pvt-repo/
    [root@ip-172-31-86-127 pvt-repo]# ls
    README.md  js_code
    [root@ip-172-31-86-127 pvt-repo]# git pull -a
    [root@ip-172-31-86-127 pvt-repo]# ls
    README.md  eks.yml  js_code
    ```

12. Install `eksctl`:
    Follow instructions from (https://eksctl.io/installation)

    ```bash
    [root@ip-172-31-86-127 pvt-repo]# ls
    README.md  eks.yml  js_code
    [root@ip-172-31-86-127 pvt-repo]# # for ARM systems, set ARCH to: `arm64`, `armv6` or `armv7`
    ARCH=amd64
    PLATFORM=$(uname -s)_$ARCH

    curl -sLO "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"

    # (Optional) Verify checksum
    curl -sL "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_checksums.txt" | grep $PLATFORM | sha256sum --check

    tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz

    sudo mv /tmp/eksctl /usr/local/bin
    eksctl_Linux_amd64.tar.gz: OK
    rm: remove regular file 'eksctl_Linux_amd64.tar.gz'? y

    [root@ip-172-31-86-127 pvt-repo]# eksctl version
    0.210.0

    ```
13. Use the `eks.yml` file to **create an EKS cluster**.

    > This takes time and will create required AWS services like nodes, VPCs, SGs, ASG, etc.

    ```bash
    [root@ip-172-31-86-127 pvt-repo]# eksctl create cluster --config-file=eks.yml
    2025-06-15 07:05:09 [ℹ]  eksctl version 0.210.0
    2025-06-15 07:05:09 [ℹ]  using region us-east-1
    2025-06-15 07:05:09 [ℹ]  setting availability zones to [us-east-1b us-east-1a]
    2025-06-15 07:05:09 [ℹ]  subnets for us-east-1b - public:192.168.0.0/19 private:192.168.64.0/19
    2025-06-15 07:05:09 [ℹ]  subnets for us-east-1a - public:192.168.32.0/19 private:192.168.96.0/19
    2025-06-15 07:05:09 [ℹ]  nodegroup "project" will use "" [AmazonLinux2023/1.32]
    2025-06-15 07:05:09 [ℹ]  using Kubernetes version 1.32
    2025-06-15 07:18:00 [✔]  EKS cluster "project" in "us-east-1" region is ready

    ```

14. Install `kubectl` to interact with the cluster.
    ```bash
    [root@ip-172-31-86-127 pvt-repo]# curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/1.33.0/2025-05-01/bin/linux/amd64/kubectl
    % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                    Dload  Upload   Total   Spent    Left  Speed
    100 57.3M  100 57.3M    0     0  21.2M      0  0:00:02  0:00:02 --:--:-- 21.2M
    [root@ip-172-31-86-127 pvt-repo]# chmod +x ./kubectl
    [root@ip-172-31-86-127 pvt-repo]# mv ./kubectl /usr/local/bin
    [root@ip-172-31-86-127 pvt-repo]# kubectl version --client
    Client Version: v1.33.0-eks-802817d
    Kustomize Version: v5.6.0
    ```
15. Test kubectl:

    ```bash
    [root@ip-172-31-86-127 pvt-repo]#  kubectl api-resources | wc -l
    66
    [root@ip-172-31-86-127 pvt-repo]# kubectl get nodes
    NAME                             STATUS   ROLES    AGE    VERSION
    ip-192-168-27-169.ec2.internal   Ready    <none>   9m7s   v1.32.3-eks-473151a
    ip-192-168-55-115.ec2.internal   Ready    <none>   9m8s   v1.32.3-eks-473151a
    ip-192-168-63-135.ec2.internal   Ready    <none>   9m7s   v1.32.3-eks-473151a
    ```

16. Create a base64 secret from Docker credentials to use in Kubernetes secrets:

    ```bash
    [root@ip-172-31-86-127 pvt-repo]# cat ~/.docker/config.json | base64 -w 0
    ewoJImF1dGhzIjxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxCn0=
    ```

17. Create a manifest file locally, push it to GitHub, and pull it on the EC2 instance.

  ![Image](https://github.com/user-attachments/assets/f09d21ff-a0c0-494f-86c4-c88338ef715e)

    ```bash
    [root@ip-172-31-86-127 pvt-repo]# cat manifest.yml
    apiVersion: v1
    kind: Secret
    metadata:
    name: regcred
    namespace: project
    type: kubernetes.io/dockerconfigjson
    data:
    .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOiB7CgkJCSJhdXRoIjogImVHRnlZWFpwYm1RNk1UazVOVjlLZFd4NSIKCQl9Cgl9Cn0=
    ---
    # Deployment
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: frontend
    namespace: project
    labels:
        app: frontend
        environment: project
    spec:
    replicas: 3
    selector:
        matchLabels:
        app: frontend
        tier: web
    template:
        metadata:
        labels:
            app: frontend
            tier: web
        spec:
        imagePullSecrets:
            - name: regcred
        containers:
            - name: frontend
            image: xaravind/jscode:v2
            ports:
                - containerPort: 80
            resources:
                requests:
                memory: "64Mi"
                cpu: "250m"
                limits:
                memory: "256Mi"
                cpu: "500m"
            readinessProbe:
                httpGet:
                path: /
                port: 80
                initialDelaySeconds: 5
                periodSeconds: 10
            livenessProbe:
                httpGet:
                path: /
                port: 80
                initialDelaySeconds: 15
                periodSeconds: 20
    ---
    # Service
    apiVersion: v1
    kind: Service
    metadata:
    name: frontend
    namespace: project
    spec:
    type: LoadBalancer
    selector:
        app: frontend
        tier: web
    ports:
        - protocol: TCP
        port: 80
        targetPort: 80
    ```
Above manifest file defines everything required to deploy a containerized application securely and publicly in a Kubernetes cluster.

 1. Docker Registry Secret

  * **Purpose:** Allows Kubernetes to pull an image from a **private Docker Hub repository** using a base64-encoded Docker config (`~/.docker/config.json`).
  * **Key:** `regcred` (referenced in the deployment under `imagePullSecrets`).

 2. Deployment

  * **Replicas:** `3` – Ensures high availability by running 3 pods.
  * **Image:** `xaravind/jscode:v1` –
  * **Probes:**
    * **Readiness Probe** – Checks if the container is ready to serve traffic.
    * **Liveness Probe** – Monitors container health and restarts if unhealthy.
  * **Resources:**
    * **Requests:** Memory: `64Mi`, CPU: `250m`
    * **Limits:** Memory: `256Mi`, CPU: `500m`
  * **Image Pull Secret:** Uses the previously created `regcred` secret to authenticate with Docker Hub.

 3. LoadBalancer Service

  * **Type:** `LoadBalancer` – Automatically provisions an **external load balancer** (like AWS ELB when running on EKS).
  * **Port Mapping:**
  
    * **port:** `80` (exposed publicly)
    * **targetPort:** `80` (mapped to the container port)

  ✅ Summary

  ✔️ **Pulls a private image** securely using a Docker Hub token

  ✔️ **Deploys the frontend application** as a replicated, monitored set of pods

  ✔️ **Uses probes** to maintain app health and availability

  ✔️ **Exposes the app** to the public internet via a load balancer


18. Create a namespace and apply the manifest file:

    ```bash
    kubectl create namespace frontend
    kubectl apply -f manifest.yaml -n frontend
    ```

19. Check pods and services.
    Kubernetes will create a **LoadBalancer service** with a random **NodePort (30000–32767)**.

    ```bash
    [root@ip-172-31-86-127 pvt-repo]# kubectl get pods -n project
    NAME                        READY   STATUS             RESTARTS      AGE
    frontend-644df5c8b4-97gbj   0/1     Running            1 (32s ago)   92s
    frontend-6467b94fb8-hghzx   0/1     Running            0             11s
    frontend-6467b94fb8-t95fs   1/1     Running            0             25s
    frontend-6d78b8b5b6-xw8hl   0/1     CrashLoopBackOff   5 (21s ago)   6m22s
    [root@ip-172-31-86-127 pvt-repo]# kubectl describe pod frontend-6467b94fb8-hghzx -n project
    Name:             frontend-6467b94fb8-hghzx

    Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                                node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
    Events:
    Type    Reason     Age   From               Message
    ----    ------     ----  ----               -------
    Normal  Scheduled  28s   default-scheduler  Successfully assigned project/frontend-6467b94fb8-hghzx to ip-192-168-63-135.ec2.internal
    Normal  Pulled     27s   kubelet            Container image "xaravind/jscode:v1" already present on machine
    Normal  Created    27s   kubelet            Created container: frontend
    Normal  Started    27s   kubelet            Started container frontend
    [root@ip-172-31-86-127 pvt-repo]# kubectl get svc frontend -n project
    NAME       TYPE           CLUSTER-IP     EXTERNAL-IP                                                              PORT(S)        AGE
    frontend   LoadBalancer   10.100.15.41   ab39bfabc67f9405f8e6c5b308273a51-706994256.us-east-1.elb.amazonaws.com   80:31689/TCP   7m3s
    ```

20. To access the app, **add inbound rule for NodePort range (30000–32767)** in the security group.

  ![Image](https://github.com/user-attachments/assets/03bf738c-017d-4b9a-bdc0-a0a2846198ab)

21. Go to EC2 → Nodes → Security Group → Edit Inbound Rules → Add port range.

  ![Image](https://github.com/user-attachments/assets/7025773d-a993-42c3-830e-a8bfe592d7dc)


22. Access the application via LoadBalancer or NodePort.

  ![Image](https://github.com/user-attachments/assets/4765df86-9029-435f-ab14-5b0a8bb919e0)

  ![Image](https://github.com/user-attachments/assets/0214b7fe-8707-406f-8bf5-2d3f573c0b46)

    ```bash
    curl http://<EXTERNAL-IP>/
    curl http://ab39bfabc67f9405f8e6c5b308273a51-706994256.us-east-1.elb.amazonaws.com/
    ````

  ![Image](https://github.com/user-attachments/assets/c1233804-81ca-4063-a6c9-0ffc4d77b451)

23. Update mainfest file with v2 image - xaravind/jscode:v2



24. repply the changes

    ```bash
    [root@ip-172-31-86-127 pvt-repo]# vi manifest.yml
    [root@ip-172-31-86-127 pvt-repo]# kubectl get pods -o wide -n project
    NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE                             NOMINATED NODE   READINESS GATES
    frontend-6467b94fb8-hghzx   1/1     Running   0          7m12s   192.168.62.210   ip-192-168-63-135.ec2.internal   <none>           <none>
    frontend-6467b94fb8-hs94j   1/1     Running   0          6m59s   192.168.43.93    ip-192-168-55-115.ec2.internal   <none>           <none>
    frontend-6467b94fb8-t95fs   1/1     Running   0          7m26s   192.168.29.173   ip-192-168-27-169.ec2.internal   <none>           <none>
    [root@ip-172-31-86-127 pvt-repo]# kubectl apply -f manifest.yml
    secret/regcred unchanged
    deployment.apps/frontend configured
    service/frontend unchanged
    [root@ip-172-31-86-127 pvt-repo]# kubectl get pods -o wide -n project
    NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE                             NOMINATED NODE   READINESS GATES
    frontend-6467b94fb8-hghzx   1/1     Running   0          7m55s   192.168.62.210   ip-192-168-63-135.ec2.internal   <none>           <none>
    frontend-6467b94fb8-hs94j   1/1     Running   0          7m42s   192.168.43.93    ip-192-168-55-115.ec2.internal   <none>           <none>
    frontend-6467b94fb8-t95fs   1/1     Running   0          8m9s    192.168.29.173   ip-192-168-27-169.ec2.internal   <none>           <none>
    frontend-f8f8b89df-zn2gn    0/1     Running   0          4s      192.168.2.38     ip-192-168-27-169.ec2.internal   <none>           <none>
    [root@ip-172-31-86-127 pvt-repo]# kubectl get pods -o wide -n project
    NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE                             NOMINATED NODE   READINESS GATES
    frontend-6467b94fb8-hghzx   1/1     Running   0          8m3s    192.168.62.210   ip-192-168-63-135.ec2.internal   <none>           <none>
    frontend-6467b94fb8-hs94j   1/1     Running   0          7m50s   192.168.43.93    ip-192-168-55-115.ec2.internal   <none>           <none>
    frontend-6467b94fb8-t95fs   1/1     Running   0          8m17s   192.168.29.173   ip-192-168-27-169.ec2.internal   <none>           <none>
    frontend-f8f8b89df-zn2gn    0/1     Running   0          12s     192.168.2.38     ip-192-168-27-169.ec2.internal   <none>           <none>
    [root@ip-172-31-86-127 pvt-repo]# kubectl get pods -o wide -n project
    NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE                             NOMINATED NODE   READINESS GATES
    frontend-6467b94fb8-hghzx   1/1     Running   0          8m10s   192.168.62.210   ip-192-168-63-135.ec2.internal   <none>           <none>
    frontend-6467b94fb8-hs94j   1/1     Running   0          7m57s   192.168.43.93    ip-192-168-55-115.ec2.internal   <none>           <none>
    frontend-f8f8b89df-ggncq    0/1     Running   0          5s      192.168.47.194   ip-192-168-63-135.ec2.internal   <none>           <none>
    frontend-f8f8b89df-zn2gn    1/1     Running   0          19s     192.168.2.38     ip-192-168-27-169.ec2.internal   <none>           <none>
    [root@ip-172-31-86-127 pvt-repo]# kubectl get pods -o wide -n project
    NAME                       READY   STATUS    RESTARTS   AGE   IP               NODE                             NOMINATED NODE   READINESS GATES
    frontend-f8f8b89df-ggncq   1/1     Running   0          61s   192.168.47.194   ip-192-168-63-135.ec2.internal   <none>           <none>
    frontend-f8f8b89df-xmjqn   1/1     Running   0          49s   192.168.39.242   ip-192-168-55-115.ec2.internal   <none>           <none>
    frontend-f8f8b89df-zn2gn   1/1     Running   0          75s   192.168.2.38     ip-192-168-27-169.ec2.internal   <none>           <none>

    ```

> ✅ Observe how Kubernetes rolls out the new version of your app without downtime – this is called a **rolling update**.

25. Access the application, we easily updated the code with zeo downtime. and with single command.

  ![Image](https://github.com/user-attachments/assets/76de6053-d79b-4a30-bb8f-a61bb678585b)

---

## ✅ Final Cleanup

* Once done, **clean up your project** to avoid charges:

  * Delete EKS cluster
  * Remove EC2 instance
  * Delete S3 buckets and Docker images if not needed

    ```bash
    eksctl delete cluster --config-file=eks.yml
    [root@ip-172-31-86-127 pvt-repo]# eksctl delete cluster --config-file=eks.yml
    2025-06-15 08:25:07 [ℹ]  deleting EKS cluster "project"
    2025-06-15 08:25:07 [ℹ]  will drain 0 unmanaged nodegroup(s) in cluster "project"
    2025-06-15 08:25:07 [ℹ]  starting parallel draining, max in-flight of 1
    2025-06-15 08:25:07 [ℹ]  deleted 0 Fargate profile(s)
    2025-06-15 08:25:08 [✔]  kubeconfig has been updated
    2025-06-15 08:25:08 [ℹ]  cleaning up AWS load balancers created by Kubernetes objects of Kind Service or Ingress
    2025-06-15 08:25:33 [ℹ]
    2 sequential tasks: { delete nodegroup "project", delete cluster control plane "project" [async]
    }
    2025-06-15 08:25:33 [ℹ]  will delete stack "eksctl-project-nodegroup-project"
    2025-06-15 08:25:33 [ℹ]  waiting for stack "eksctl-project-nodegroup-project" to get deleted
    2025-06-15 08:25:33 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:26:03 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:26:48 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:27:40 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:28:30 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:29:31 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:30:55 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:32:25 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:34:07 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:35:46 [ℹ]  waiting for CloudFormation stack "eksctl-project-nodegroup-project"
    2025-06-15 08:35:46 [ℹ]  will delete stack "eksctl-project-cluster"
    2025-06-15 08:35:46 [✔]  all cluster resources were deleted
    ```

> ✅ In this stage, we **created an IAM role** with Administrator access, **attached the role** to the EC2 instance, **installed `eksctl` and `kubectl` packages**, **created an EKS cluster**, **set up a namespace**, **applied the manifest file**, **accessed the application**, **updated the deployment with image v2**, and finally **performed cleanup** of all resources.

